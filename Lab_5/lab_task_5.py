# -*- coding: utf-8 -*-
"""Lab_Task_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nSGR9fsdkdjMdSeMhjVVYb3rh9Kv_UTo
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

df_k_means = pd.read_csv('/content/drive/MyDrive/CSE475/Lab_05/Mall_Customers.csv')
df_pca = pd.read_csv('/content/drive/MyDrive/CSE475/Lab_05/Iris.csv')
df_apriori = pd.read_csv('/content/drive/MyDrive/CSE475/Lab_05/Groceries_dataset.csv')

"""# **K Means Clustering**"""

df_k_means

df_k_means.isnull().sum()

df_k_means.duplicated().sum()

df_k_means.info()

df_k_means.columns.to_list()

# Scatter Plot
x = df_k_means['Age']
y = df_k_means['Annual Income (k$)']

plt.xlabel('Age')
plt.ylabel('Annual Income (k$)')

plt.scatter(x, y)
plt.show()

# Elbow Method

data = list(zip(x, y))
inertias = []

for i in range(1,11):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(data)
    inertias.append(kmeans.inertia_)

plt.plot(range(1,11), inertias, marker='o')
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

# Clustering

kmeans = KMeans(n_clusters=4)
kmeans.fit(data)

plt.scatter(x, y, c=kmeans.labels_)
plt.show()

"""# **Apriori Algorithm**"""

df_apriori

df_apriori.isnull().sum()

df_apriori.duplicated().sum()

df_apriori.drop_duplicates(inplace=True)

df_apriori.duplicated().sum()

df_apriori.info()

df_apriori['itemDescription'] = df_apriori['itemDescription'].str.strip()

df_apriori

basket = pd.pivot_table(data=df_apriori,index='Member_number',columns='itemDescription',values='itemDescription', aggfunc='sum',fill_value=0)

basket

basket['yogurt'].head(10)

def convert_into_binary(x):
    if x != 0:
        return 1
    else:
        return 0

basket_sets = basket.applymap(convert_into_binary)

basket_sets['yogurt'].head(10)

frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)

frequent_itemsets

rules_mlxtend = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
rules_mlxtend.head()

rules_mlxtend[ (rules_mlxtend['lift'] >= 1) & (rules_mlxtend['confidence'] >= 0.3) ].head()

"""# **PCA Analysis**"""

df_pca

df_pca.isnull().sum()

df_pca.duplicated().sum()

df_pca.info()

df_pca.Species.unique()

mappings = {
    "Species": {"Iris-setosa": 1,
                "Iris-versicolor": 2,
                "Iris-virginica": 3}
}

df_pca.replace(mappings, inplace=True)

df_pca

df_pca.drop(columns=['Id'], inplace=True)

X=df_pca.drop(columns=['Species'])
y=df_pca['Species']

# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Applying PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Explained variance ratio
print("Explained variance ratio:", pca.explained_variance_ratio_)

# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Plot the PCA results
plt.figure(figsize=(8,6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='coolwarm', edgecolors='k', alpha=0.7)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA Visualization")
plt.colorbar()
plt.show()